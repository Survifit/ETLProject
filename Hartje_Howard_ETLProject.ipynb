{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Project\n",
    "<ul>\n",
    "    <li>UofMN Data Visualization and Analytics Bootcamp</li>\n",
    "    <li>Week 13 | ETL Project</li>\n",
    "    <li>Created by: Stephanie Hartje, Chris Howard</li>\n",
    "    <li>05/18/2019</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Description and Purpose\n",
    "<p>This project extracts(E) data from multiple sources, uses the Python Pandas module to transform(T) the data into \n",
    "    useful tables, which can then be mapped and loaded(L) into a SQL database. There is no direct analysis done on\n",
    "    the data for the project, but the intention is to have a usable database for a theoretical analysis at the end of \n",
    "    the process.</p>\n",
    "<p>Our theoretical analysis is looking at any (albeit spurious) correlation between solar eclipses, ufo sightings, and \n",
    "    multiple natural disasters including hurricanes and earthquakes. Each event type has been given its own table \n",
    "    in the database with a minimum of event date, some form of ID, and location (including latitude and longitude where\n",
    "    available. All dates have been separated into 'year' 'month' 'day' columns so that events can be easily \n",
    "    compared by for clusters around certain months as well as by year and location.</p>\n",
    "<p>The sql code for our database can be found in our repository, or opened directly into a new Jupyter window <a href='../edit/disaster_etl.sql'>using this link</a> if this notebook is being run locally within a copy of the repository.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chris Extract/Transform below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ufo data from wikipedia, data from 19th & 20th \n",
    "ufo_url = 'https://en.wikipedia.org/wiki/List_of_reported_UFO_sightings'\n",
    "ufo_df_19th = pd.read_html(ufo_url)[5]\n",
    "ufo_df_20th = pd.read_html(ufo_url)[6]\n",
    "\n",
    "# remove label row from 20th century data\n",
    "ufo_df_20th = ufo_df_20th.drop(0)\n",
    "\n",
    "# combine tables into single dataframe\n",
    "ufo_df = ufo_df_19th.append(ufo_df_20th, ignore_index=True) \n",
    "\n",
    "# use first row as column headers, then reindex removing top row\n",
    "ufo_df.columns = ufo_df.iloc[0]\n",
    "ufo_df = ufo_df.reindex(ufo_df.index.drop(0))\n",
    "\n",
    "# create loop to extract year/month/day from formatting\n",
    "dates = ufo_df['Date']\n",
    "year = []\n",
    "month = []\n",
    "day = []\n",
    "for date in dates:\n",
    "    date = date.strip('s')\n",
    "    date = date.split('-')\n",
    "    year.append(date[0])\n",
    "    if len(date) > 1:\n",
    "        month.append(date[1])\n",
    "    else:\n",
    "        month.append(None)\n",
    "    if len(date) > 2:\n",
    "        day.append(date[2])\n",
    "    else:\n",
    "        day.append(None)\n",
    "\n",
    "# insert 'Year' 'Month' 'Day' columns into the dataframe\n",
    "ufo_df.insert(loc=0, column='Year', value=year)\n",
    "ufo_df.insert(loc=1, column='Month', value=month)\n",
    "ufo_df.insert(loc=2, column='Day', value=day)\n",
    "ufo_df_clean = ufo_df[['Year', 'Month', 'Day', 'Date', 'Name', 'Country', 'Description']].copy()\n",
    "ufo_df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_1900 = pd.read_csv('Data/1901-2000.csv', index_col=False)\n",
    "eclipse_2000 = pd.read_csv('Data/2001-2100.csv', index_col=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stephanie Extract/Transform below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract CSVs into DataFrames\n",
    "    ### AtlanticStorms from https://www.kaggle.com/noaa/hurricane-database\n",
    "        #### Each date has up to 5 observations per day (but not all days have 5)\n",
    "        #### Older data appears to use -999 from wind pressure and speed instead of something like NA\n",
    "        #### ID: AL = Atlantic, XX = number storm for year, YYYY = year\n",
    "    ### PacificStorms from https://www.kaggle.com/noaa/hurricane-database\n",
    "        #### Each date has up to 5 observations per day (but not all days have 5)\n",
    "        #### Older data appears to use -999 from wind pressure and speed instead of something like NA\n",
    "        #### ID: EP = Pacific, XX = number storm for year, YYYY = year\n",
    "    ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extract Atlantic Storm Data\n",
    "\n",
    "AtlanticStorm = \"Data/Atlantic_Storms.csv\"\n",
    "AtlanticStorm_df = pd.read_csv(AtlanticStorm)\n",
    "AtlanticStorm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Pacific Storm Data\n",
    "\n",
    "PacificStorm = \"Data/Pacific_Storms.csv\"\n",
    "PacificStorm_df = pd.read_csv(PacificStorm)\n",
    "PacificStorm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Atlantic and Pacific Storm Data\n",
    "\n",
    "AtlPacStorms = [AtlanticStorm_df, PacificStorm_df]\n",
    "AtlPacStorms_df = pd.concat(AtlPacStorms).reset_index(drop=True)\n",
    "AtlPacStorms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that Pacific Storms are included in combined df\n",
    "\n",
    "AtlPacStorms_df.loc[AtlPacStorms_df['ID'] == \"EP011949\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AtlPacStorms_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust date format\n",
    "\n",
    "# make string version of original Date column, call it 'col'\n",
    "AtlPacStorms_df['col'] = AtlPacStorms_df['Date'].apply(str)\n",
    "\n",
    "# make the new columns using string indexing\n",
    "AtlPacStorms_df['Year'] = AtlPacStorms_df['col'].str[0:4]\n",
    "AtlPacStorms_df['Month'] = AtlPacStorms_df['col'].str[4:6]\n",
    "AtlPacStorms_df['Day'] = AtlPacStorms_df['col'].str[6:8]\n",
    "\n",
    "# get rid of the extra variable (if you want)\n",
    "AtlPacStorms_df.drop('col', axis=1, inplace=True)\n",
    "\n",
    "#check result\n",
    "AtlPacStorms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select columns to keep\n",
    "\n",
    "AtlPacStorms_df = AtlPacStorms_df[[\"Year\", \"Month\", \"Day\", \"ID\", \"Status\", \"Time\", \"Latitude\", \"Longitude\"]]\n",
    "AtlPacStorms_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AtlPacStorms_df[\"Status\"] = AtlPacStorms_df['Status'].astype(str)\n",
    "AtlPacStorms_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only interested in Hurricanes to only keep rows with Status = HU\n",
    "\n",
    "AtlPacStorms_df = AtlPacStorms_df.loc[AtlPacStorms_df[\"Status\"] == \" HU\"]\n",
    "AtlPacStorms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the first observation of each unique ID\n",
    "\n",
    "Hurricane_df = AtlPacStorms_df.drop_duplicates(subset=[AtlPacStorms_df.columns[3]], keep = \"first\")\n",
    "Hurricane_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Status and Time columns\n",
    "\n",
    "Hurricane_df = Hurricane_df[[\"Year\", \"Month\", \"Day\", \"ID\", \"Latitude\", \"Longitude\"]]\n",
    "Hurricane_df = Hurricane_df.reset_index(drop = True)\n",
    "Hurricane_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chris Load below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = f\"{config.username}:{config.password}@127.0.0.1/disaster_etl\"\n",
    "engine = create_engine(f'mysql+pymysql://{conn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo_df_clean.to_sql(name='ufo_sightings', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stephanie Load below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
