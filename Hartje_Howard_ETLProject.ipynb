{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Project\n",
    "<ul>\n",
    "    <li>UofMN Data Visualization and Analytics Bootcamp</li>\n",
    "    <li>Week 13 | ETL Project</li>\n",
    "    <li>Created by: Stephanie Hartje, Chris Howard</li>\n",
    "    <li>05/18/2019</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Description and Purpose\n",
    "<p>This project extracts(E) data from multiple sources, uses the Python Pandas module to transform(T) the data into \n",
    "    useful tables, which can then be mapped and loaded(L) into a SQL database. There is no direct analysis done on\n",
    "    the data for the project, but the intention is to have a usable database for a theoretical analysis at the end of \n",
    "    the process.</p>\n",
    "<p>Our theoretical analysis is looking at any (albeit spurious) correlation between solar eclipses, ufo sightings, and \n",
    "    multiple natural disasters including hurricanes and volcanoes. Each event type has been given its own table \n",
    "    in the database with a minimum of event date, some form of ID, and location (including latitude and longitude where\n",
    "    available). All dates have been separated into 'year' 'month' 'day' columns so that events can be easily \n",
    "    compared by date for clusters around certain months as well as by year and location.</p>\n",
    "<p>The sql code for our database can be found in our repository, or opened directly into a new Jupyter window <a href='../edit/disaster_etl.sql'>using this link</a> if this notebook is being run locally within a copy of the repository.</p>\n",
    "<p>Extract:</p>\n",
    "     <p>We extracted the following:</p>\n",
    "        <ol><li>Solar Eclipses</li>\n",
    "            <ul><li>From: https://data.world/nasa/five-millennium-catalog-of-solar-eclipses-detailed</li>\n",
    "                <li>Original Format: CSV</li></ul>\n",
    "        <li>UFO Sightings</li>\n",
    "            <ul><li>From: https://en.wikipedia.org/wiki/List_of_reported_UFO_sightings</li>\n",
    "                <li>Original Format: 2 HTML tables, one for 19th century and one for 20th century</li></ul>\n",
    "        <li>Hurricanes</li>\n",
    "            <ul><li>From: https://www.kaggle.com/noaa/hurricane-database</li>\n",
    "                <li>Original Format: Two CSV files, one for Atlantic and one for Pacific Storms</li></ul>\n",
    "        <li>Volcanoes:</li>\n",
    "            <ul><li>From:  https://data.world/dhs/historical-significant</li>\n",
    "                <li>Original Format: CSV</li></ul></ol>\n",
    "<p>Transform:</p>\n",
    "    <p>We performed the following transformation steps for each data set respectively:</p>\n",
    "        <ol><li>Solar Eclipses</li>\n",
    "            <ul><li></li></ul>\n",
    "        <li>UFO Sightings</li>\n",
    "            <ul><li>remove label row from 20th century table and combine into one table</li>\n",
    "                <li>use first row as header and re-index</li>\n",
    "                <li>separate year, month, and date into separate columns</li>\n",
    "                <li>select the columns to keep</li></ul>\n",
    "        <li>Hurricanes</li>\n",
    "            <ul><li>combine Atlantic and Pacific tables into one table</li>\n",
    "            <li>convert date column to string and separate year, month, and day into separate columns</li>\n",
    "            <li>select the columns to keep</li>\n",
    "            <li>select the rows corresponding to Hurricane data</li>\n",
    "            <li>keep only the first observation related to a particular Hurricane</li>\n",
    "            <li>convert latitude and longitude to format consistent with other tables</li></ul>\n",
    "        <li>Volcanoes</li>\n",
    "            <ul><li>select columns to keep</li>\n",
    "                <li>rename columns</li></ul></ol>\n",
    "<p>Load:</p>\n",
    "    <p>We created a database in MySQL (disaster_etl) and created a table for each dataset.  We then loaded the data into MySQL using sqlalchemy.</p>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chris Extract/Transform below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ufo data from wikipedia, data from 19th & 20th \n",
    "ufo_url = 'https://en.wikipedia.org/wiki/List_of_reported_UFO_sightings'\n",
    "ufo_df_19th = pd.read_html(ufo_url)[5]\n",
    "ufo_df_20th = pd.read_html(ufo_url)[6]\n",
    "\n",
    "# remove label row from 20th century data\n",
    "ufo_df_20th = ufo_df_20th.drop(0)\n",
    "\n",
    "# combine tables into single dataframe\n",
    "ufo_df = ufo_df_19th.append(ufo_df_20th, ignore_index=True) \n",
    "\n",
    "# use first row as column headers, then reindex removing top row\n",
    "ufo_df.columns = ufo_df.iloc[0]\n",
    "ufo_df = ufo_df.reindex(ufo_df.index.drop(0))\n",
    "\n",
    "# create loop to extract year/month/day from formatting\n",
    "dates = ufo_df['Date']\n",
    "year = []\n",
    "month = []\n",
    "day = []\n",
    "for date in dates:\n",
    "    date = date.strip('s')\n",
    "    date = date.split('-')\n",
    "    year.append(date[0])\n",
    "    if len(date) > 1:\n",
    "        month.append(date[1])\n",
    "    else:\n",
    "        month.append(None)\n",
    "    if len(date) > 2:\n",
    "        day.append(date[2])\n",
    "    else:\n",
    "        day.append(None)\n",
    "\n",
    "# insert 'Year' 'Month' 'Day' columns into the dataframe\n",
    "ufo_df.insert(loc=0, column='Year', value=year)\n",
    "ufo_df.insert(loc=1, column='Month', value=month)\n",
    "ufo_df.insert(loc=2, column='Day', value=day)\n",
    "ufo_df_clean = ufo_df[['Year', 'Month', 'Day', 'Date', 'Name', 'Country', 'Description']].copy()\n",
    "ufo_df_clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eclipse data, first needed manual cleaning in .csv files to remove extra columns from random rows\n",
    "eclipse_1900 = pd.read_csv('Data/1901-2000.csv', index_col=False)\n",
    "eclipse_2000 = pd.read_csv('Data/2001-2100.csv', index_col=False)\n",
    "eclipse_df = eclipse_1900.append(eclipse_2000)\n",
    "\n",
    "eclipse_df_clean = eclipse_df[['Catalog Number', 'Calendar Year', 'Calendar Month', 'Calendar Day', 'Ecl. Type',\n",
    "                              u'Lat \\N{DEGREE SIGN}', u'Long \\N{DEGREE SIGN}']]\n",
    "\n",
    "eclipse_df_clean = eclipse_df_clean.rename(columns={'Catalog Number':'catalog_number', \n",
    "                                 'Calendar Year': 'year', \n",
    "                                 'Calendar Month': 'month_old', \n",
    "                                 'Calendar Day': 'day', \n",
    "                                 'Ecl. Type': 'eclipse_type', \n",
    "                                 u'Lat \\N{DEGREE SIGN}': 'latitude_old', \n",
    "                                 u'Long \\N{DEGREE SIGN}': 'longitude_old'})\n",
    "\n",
    "latitude = eclipse_df_clean['latitude_old']\n",
    "new_lat = []\n",
    "for lat in latitude:\n",
    "    \n",
    "    if lat[-1] == 'S':\n",
    "        lat = lat[:-1]\n",
    "        lat = '-' + ''.join(lat)\n",
    "    else:\n",
    "        lat = lat[:-1]\n",
    "        lat = ''.join(lat)\n",
    "    new_lat.append(lat)\n",
    "\n",
    "longitude = eclipse_df_clean['longitude_old']\n",
    "new_lon = []\n",
    "for lon in longitude:\n",
    "    if lon[-1] == 'W':\n",
    "        lon = lon[:-1]\n",
    "        lon = '-' + ''.join(lon)\n",
    "    else:\n",
    "        lon = lon[:-1]\n",
    "        lon = ''.join(lon)\n",
    "    new_lon.append(lon)\n",
    "\n",
    "eclipse_df_clean['latitude'] = new_lat\n",
    "eclipse_df_clean['longitude'] = new_lon\n",
    "eclipse_df_clean = eclipse_df_clean.drop(columns=['latitude_old', 'longitude_old'])\n",
    "\n",
    "months = eclipse_df_clean['month_old']\n",
    "months_new = []\n",
    "def months_to_numbers(argument): \n",
    "    switcher = { \n",
    "        'Jan': '01', \n",
    "        'Feb': '02', \n",
    "        'Mar': '03',\n",
    "        'Apr': '04',\n",
    "        'May': '05',\n",
    "        'Jun': '06',\n",
    "        'Jul': '07',\n",
    "        'Aug': '08',\n",
    "        'Sep': '09',\n",
    "        'Oct': '10',\n",
    "        'Nov': '11',\n",
    "        'Dec': '12'\n",
    "    } \n",
    "    return switcher.get(argument, \"nothing\") \n",
    "\n",
    "for month in months:\n",
    "    month = months_to_numbers(month)\n",
    "    months_new.append(month)\n",
    "    \n",
    "eclipse_df_clean.insert(loc=2, column='month', value=months_new)\n",
    "eclipse_df_clean = eclipse_df_clean.drop(columns=['month_old'])\n",
    "eclipse_df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stephanie Extract/Transform below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract CSVs into DataFrames\n",
    "    AtlanticStorms from https://www.kaggle.com/noaa/hurricane-database\n",
    "        - Each date has up to 5 observations per day (but not all days have 5)\n",
    "        - Older data appears to use -999 from wind pressure and speed instead of something like NA\n",
    "        - ID: AL = Atlantic, XX = number storm for year, YYYY = year\n",
    "    PacificStorms from https://www.kaggle.com/noaa/hurricane-database\n",
    "        - Each date has up to 5 observations per day (but not all days have 5)\n",
    "        - Older data appears to use -999 from wind pressure and speed instead of something like NA\n",
    "        - ID: EP = Pacific, XX = number storm for year, YYYY = year\n",
    "    Volcanoes from https://data.world/dhs/historical-significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Extract Atlantic Storm Data\n",
    "\n",
    "AtlanticStorm = \"Data/Atlantic_Storms.csv\"\n",
    "AtlanticStorm_df = pd.read_csv(AtlanticStorm)\n",
    "AtlanticStorm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Pacific Storm Data\n",
    "\n",
    "PacificStorm = \"Data/Pacific_Storms.csv\"\n",
    "PacificStorm_df = pd.read_csv(PacificStorm)\n",
    "PacificStorm_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform Hurricane Data\n",
    "# Combine Atlantic and Pacific Storm Data\n",
    "\n",
    "AtlPacStorms = [AtlanticStorm_df, PacificStorm_df]\n",
    "AtlPacStorms_df = pd.concat(AtlPacStorms).reset_index(drop=True)\n",
    "AtlPacStorms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that Pacific Storms are included in combined df\n",
    "\n",
    "AtlPacStorms_df.loc[AtlPacStorms_df['ID'] == \"EP011949\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AtlPacStorms_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust date format\n",
    "\n",
    "# make string version of original Date column, call it 'col'\n",
    "AtlPacStorms_df['col'] = AtlPacStorms_df['Date'].apply(str)\n",
    "\n",
    "# make the new columns using string indexing\n",
    "AtlPacStorms_df['Year'] = AtlPacStorms_df['col'].str[0:4]\n",
    "AtlPacStorms_df['Month'] = AtlPacStorms_df['col'].str[4:6]\n",
    "AtlPacStorms_df['Day'] = AtlPacStorms_df['col'].str[6:8]\n",
    "\n",
    "# get rid of the extra variable (if you want)\n",
    "AtlPacStorms_df.drop('col', axis=1, inplace=True)\n",
    "\n",
    "#check result\n",
    "AtlPacStorms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select columns to keep\n",
    "\n",
    "AtlPacStorms_df = AtlPacStorms_df[[\"Year\", \"Month\", \"Day\", \"ID\", \"Status\", \"Time\", \"Latitude\", \"Longitude\"]]\n",
    "AtlPacStorms_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AtlPacStorms_df[\"Status\"] = AtlPacStorms_df['Status'].astype(str)\n",
    "AtlPacStorms_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are only interested in Hurricanes to only keep rows with Status = HU\n",
    "\n",
    "AtlPacStorms_df = AtlPacStorms_df.loc[AtlPacStorms_df[\"Status\"] == \" HU\"]\n",
    "AtlPacStorms_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the first observation of each unique ID\n",
    "\n",
    "Hurricane_df = AtlPacStorms_df.drop_duplicates(subset=[AtlPacStorms_df.columns[3]], keep = \"first\")\n",
    "Hurricane_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Status and Time columns\n",
    "\n",
    "Hurricane_df = Hurricane_df[[\"Year\", \"Month\", \"Day\", \"ID\", \"Latitude\", \"Longitude\"]]\n",
    "Hurricane_df = Hurricane_df.reset_index(drop = True)\n",
    "Hurricane_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-format Latitude and Longitude to align with format in other tables\n",
    "#rename columns\n",
    "Hurricane_df.columns = [\"Year\", \"Month\", \"Day\", \"ID\", \"latitude_old\", \"longitude_old\"]\n",
    "Hurricane_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change format\n",
    "latitude = Hurricane_df['latitude_old']\n",
    "new_lat = []\n",
    "for lat in latitude:\n",
    "\n",
    "   if lat[-1] == 'S':\n",
    "       lat = lat[:-1]\n",
    "       lat = '-' + ''.join(lat)\n",
    "   else:\n",
    "       lat = lat[:-1]\n",
    "       lat = ''.join(lat)\n",
    "   new_lat.append(lat)\n",
    "\n",
    "longitude = Hurricane_df['longitude_old']\n",
    "new_lon = []\n",
    "for lon in longitude:\n",
    "   if lon[-1] == 'W':\n",
    "       lon = lon[:-1]\n",
    "       lon = '-' + ''.join(lon)\n",
    "   else:\n",
    "       lon = lon[:-1]\n",
    "       lon = ''.join(lon)\n",
    "   new_lon.append(lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add new lists to dataframe\n",
    "Hurricane_df['Latitude'] = new_lat\n",
    "Hurricane_df['Longitude'] = new_lon\n",
    "Hurricane_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop old columns\n",
    "Hurricane_df = Hurricane_df[['Year','Month','Day','ID','Latitude','Longitude']]\n",
    "Hurricane_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Volcano Data\n",
    "\n",
    "Volcano = \"Data/Historical_Significant_Volcanic_Eruption_Locations.csv\"\n",
    "Volcano_df = pd.read_csv(Volcano)\n",
    "Volcano_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform Volcano Data\n",
    "#Select columns\n",
    "\n",
    "Volcano_df = Volcano_df[[\"YEAR\", \"MO\", \"DAY\", \"VOL_ID\", \"NAME\", \"LOCATION\", \"LATITUDE\", \"LONGITUDE\"]]\n",
    "Volcano_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename Columns\n",
    "\n",
    "Volcano_df.columns = ['Year', 'Month', 'Day', 'Volcano_ID', 'Volcano_Name', 'Location', 'Latitude', 'Longitude']\n",
    "Volcano_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Chris Load below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = f\"{config.username}:{config.password}@127.0.0.1/disaster_etl\"\n",
    "engine = create_engine(f'mysql+pymysql://{conn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ufo_df_clean.to_sql(name='ufo_sightings', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_df_clean.to_sql(name='eclipse_event', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Stephanie Load below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = f\"{config.username}:{config.password}@127.0.0.1/disaster_etl\"\n",
    "engine = create_engine(f'mysql+pymysql://{conn}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.table_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Volcano_df.to_sql(name='volcano_eruptions', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hurricane_df.to_sql(name='hurricanes', con=engine, if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
